{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hansoo/.pyenv/versions/3.9.15/envs/fire/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os, collections, random, tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(''))))\n",
    "from snapbot_env.class_snapbot import Snapbot4EnvClass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ReplayBuffer` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferClass():\n",
    "    def __init__(self,\n",
    "                 buffer_limit,\n",
    "                 device) -> None:\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "        self.device = device\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def put(self,\n",
    "            item):\n",
    "        self.buffer.append(item)\n",
    "    \n",
    "    def put_mini_batch(self,\n",
    "            mini_batch):\n",
    "        for transition in mini_batch:\n",
    "            self.put(transition)\n",
    "        \n",
    "    def sample(self,\n",
    "               n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done_mask else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "        return torch.tensor(np.array(s_lst), dtype=torch.float).to(self.device), \\\n",
    "                 torch.tensor(np.array(a_lst), dtype=torch.float).to(self.device), \\\n",
    "                     torch.tensor(np.array(r_lst), dtype=torch.float).to(self.device), \\\n",
    "                         torch.tensor(np.array(s_prime_lst), dtype=torch.float).to(self.device), \\\n",
    "                             torch.tensor(np.array(done_mask_lst), dtype=torch.float).to(self.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Actor` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorClass(nn.Module):\n",
    "    def __init__(self,\n",
    "                 name = \"actor\",\n",
    "                 obs_dim = 8,\n",
    "                 h_dims = [256, 256],\n",
    "                 out_dim = 1,\n",
    "                 max_torque = 1,\n",
    "                 init_alpha = 0.1,\n",
    "                 lr_actor = 0.0003,\n",
    "                 lr_alpha = 0.0003,\n",
    "                 device = None) -> None:\n",
    "        super(ActorClass, self).__init__()\n",
    "        # Initialize\n",
    "        self.name = name\n",
    "        self.obs_dim = obs_dim\n",
    "        self.h_dims = h_dims\n",
    "        self.out_dim = out_dim\n",
    "        self.max_torque = max_torque\n",
    "        self.init_alpha = init_alpha\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_alpha = lr_alpha\n",
    "        self.device = device\n",
    "        self.init_layers()\n",
    "        self.init_params()\n",
    "        # Set optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.parameters(), lr=self.lr_actor)\n",
    "        self.log_alpha = torch.tensor(np.log(self.init_alpha), requires_grad=True, device=self.device)\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=self.lr_alpha)\n",
    "        \n",
    "    def init_layers(self):\n",
    "        self.layers = {}\n",
    "        h_dim_prev = self.obs_dim\n",
    "        for h_idx, h_dim in enumerate(self.h_dims):\n",
    "            self.layers['mlp_{}'.format(h_idx)] = nn.Linear(h_dim_prev, h_dim)\n",
    "            self.layers['relu_{}'.format(h_idx)] = nn.ReLU()\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['mu'] = nn.Linear(h_dim_prev, self.out_dim)\n",
    "        self.layers['std'] = nn.Linear(h_dim_prev, self.out_dim)\n",
    "\n",
    "        self.param_dict = {}\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                self.param_dict[key+'_w'] = layer.weight\n",
    "                self.param_dict[key+'_b'] = layer.bias\n",
    "        self.parameters = nn.ParameterDict(self.param_dict)\n",
    "\n",
    "    def init_params(self):\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                nn.init.normal_(layer.weight,mean=0.0,std=0.01)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            elif isinstance(layer,nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight,1.0)\n",
    "                nn.init.constant_(layer.bias,0.0)\n",
    "            elif isinstance(layer,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "                \n",
    "    def forward(self,\n",
    "                x):\n",
    "        x = x.to(self.device)\n",
    "        for h_idx, _ in enumerate(self.h_dims):\n",
    "            x = self.layers['relu_{}'.format(h_idx)](self.layers['mlp_{}'.format(h_idx)](x))\n",
    "        mean = self.layers['mu'](x)\n",
    "        std = F.softplus(self.layers['std'](x)) + 1e-5\n",
    "        GaussianDistribution = Normal(mean, std)\n",
    "        action = GaussianDistribution.rsample()\n",
    "        log_prob = GaussianDistribution.log_prob(action)\n",
    "        real_action = torch.tanh(action) * self.max_torque\n",
    "        real_log_prob = log_prob - torch.log(self.max_torque*(1-torch.tanh(action).pow(2)) + 1e-6)\n",
    "        return real_action, real_log_prob\n",
    "    \n",
    "    def train(self,\n",
    "              q_1,\n",
    "              q_2,\n",
    "              target_entropy,\n",
    "              mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "        \n",
    "        q_1_value = q_1(s, a)\n",
    "        q_2_value = q_2(s, a)\n",
    "        q_1_q_2_value = torch.cat([q_1_value, q_2_value], dim=1)\n",
    "        min_q_value = torch.min(q_1_q_2_value, 1, keepdim=True)[0]\n",
    "        \n",
    "        actor_loss = -min_q_value - entropy\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.mean().backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob+target_entropy).detach()).mean()\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Critic` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticClass(nn.Module):\n",
    "    def __init__(self,\n",
    "                 name = \"critic\",\n",
    "                 obs_dim = 75,\n",
    "                 a_dim = 8,\n",
    "                 h_dims = [256, 256],\n",
    "                 out_dim = 1,\n",
    "                 lr_critic = 0.0003,\n",
    "                 device = None) -> None:\n",
    "        super(CriticClass, self).__init__()\n",
    "        # Initialize\n",
    "        self.name = name\n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.h_dims = h_dims\n",
    "        self.out_dim = out_dim\n",
    "        self.lr_critic = lr_critic\n",
    "        self.device = device\n",
    "        self.init_layers()\n",
    "        self.init_params()\n",
    "        # Set optimizer\n",
    "        self.critic_optimizer = optim.Adam(self.parameters(), lr=self.lr_critic)\n",
    "\n",
    "    def init_layers(self):\n",
    "        self.layers = {}\n",
    "        h_dim_prev = self.h_dims[0]\n",
    "        for h_idx, h_dim in enumerate(self.h_dims):\n",
    "            if h_idx == 0:\n",
    "                self.layers['obs'] = nn.Linear(self.obs_dim, int(self.h_dims[0]/2))\n",
    "                self.layers['obs_relu'] = nn.ReLU()\n",
    "                self.layers['act'] = nn.Linear(self.a_dim, int(self.h_dims[0]/2))\n",
    "                self.layers['act_relu'] = nn.ReLU()\n",
    "            else:\n",
    "                self.layers['mlp_{}'.format(h_idx)] = nn.Linear(h_dim_prev, h_dim)\n",
    "                self.layers['relu_{}'.format(h_idx)] = nn.ReLU()\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['out'] = nn.Linear(h_dim_prev, self.out_dim)\n",
    "\n",
    "        self.param_dict = {}\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                self.param_dict[key+'_w'] = layer.weight\n",
    "                self.param_dict[key+'_b'] = layer.bias\n",
    "        self.parameters = nn.ParameterDict(self.param_dict)\n",
    "\n",
    "    def init_params(self):\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                nn.init.normal_(layer.weight,mean=0.0,std=0.01)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            elif isinstance(layer,nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight,1.0)\n",
    "                nn.init.constant_(layer.bias,0.0)\n",
    "            elif isinstance(layer,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "                \n",
    "    def forward(self,\n",
    "                x,\n",
    "                a):\n",
    "        x = x.to(self.device)\n",
    "        a = a.to(self.device)\n",
    "        for h_idx, _ in enumerate(self.h_dims):\n",
    "            if h_idx == 0:\n",
    "                x = self.layers['obs_relu'](self.layers['obs'](x))\n",
    "                a = self.layers['act_relu'](self.layers['act'](a))\n",
    "                cat = torch.cat([x,a], dim=1)\n",
    "            else:\n",
    "                 q = self.layers['relu_{}'.format(h_idx)](self.layers['mlp_{}'.format(h_idx)](cat))\n",
    "        q = self.layers['out'](q)\n",
    "        return q\n",
    "    \n",
    "    def train(self,\n",
    "              target,\n",
    "              mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        critic_loss = F.smooth_l1_loss(self.forward(s,a), target)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.mean().backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "    def soft_update(self, tau, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Util` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(pi, q1, q2, gamma, mini_batch, device):\n",
    "    q1 = q1.to(device)\n",
    "    q2 = q2.to(device)\n",
    "    pi = pi.to(device)\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy.mean())\n",
    "    return target "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Env` & `Hyperparameter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapbot(4legs) Environment\n",
      "Obs Dim: [103] Act Dim: [8] dt:[0.02] Condition:[None]\n",
      "ctrl_coef:[0] head_coef:[0]\n"
     ]
    }
   ],
   "source": [
    "env = Snapbot4EnvClass(xml_path='../snapbot_env/xml/snapbot_4/robot_4_', render_mode=None)\n",
    "epi_length = 300\n",
    "max_torque = 1.0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_episode = 500\n",
    "random_episode = 10\n",
    "buffer_limit = 10000000\n",
    "n_gradient_step_per_update = 1\n",
    "lr_critic = 0.0003\n",
    "lr_actor = 0.0003\n",
    "lr_alpha = 0.0003\n",
    "init_alpha = 0.1\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 256\n",
    "\n",
    "print_interval = 20\n",
    "save_interval = 100\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Main` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/500 [00:05<03:15,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 20, REWARD: 0.12, XDIFF: 0.00, ALPHA: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40/500 [00:28<19:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 40, REWARD: 4.88, XDIFF: 0.10, ALPHA: 0.0638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/500 [01:24<20:31,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 60, REWARD: 11.79, XDIFF: 0.24, ALPHA: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 80/500 [02:19<19:02,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 80, REWARD: 2.16, XDIFF: 0.04, ALPHA: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [03:14<18:46,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 100, REWARD: 23.76, XDIFF: 0.48, ALPHA: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 120/500 [04:11<17:43,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 120, REWARD: 77.17, XDIFF: 1.54, ALPHA: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140/500 [05:05<16:35,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 140, REWARD: 114.87, XDIFF: 2.30, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 160/500 [06:00<15:28,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 160, REWARD: 87.80, XDIFF: 1.76, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 180/500 [06:59<16:02,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 180, REWARD: 99.66, XDIFF: 1.99, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [07:59<14:43,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 200, REWARD: 134.27, XDIFF: 2.69, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 220/500 [08:58<13:44,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 220, REWARD: 147.00, XDIFF: 2.94, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240/500 [10:00<13:48,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 240, REWARD: 149.77, XDIFF: 3.00, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 260/500 [11:03<12:58,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 260, REWARD: 153.03, XDIFF: 3.06, ALPHA: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 280/500 [12:05<11:08,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 280, REWARD: 160.83, XDIFF: 3.22, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [13:06<10:13,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 300, REWARD: 165.67, XDIFF: 3.31, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 320/500 [14:06<08:52,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 320, REWARD: 171.36, XDIFF: 3.43, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 340/500 [15:06<07:59,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 340, REWARD: 174.13, XDIFF: 3.48, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 360/500 [16:08<07:21,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 360, REWARD: 170.82, XDIFF: 3.42, ALPHA: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 380/500 [17:07<05:36,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 380, REWARD: 169.59, XDIFF: 3.39, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [18:02<04:41,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 400, REWARD: 178.06, XDIFF: 3.56, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 420/500 [18:57<03:38,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 420, REWARD: 179.33, XDIFF: 3.59, ALPHA: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 440/500 [19:52<02:44,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 440, REWARD: 181.15, XDIFF: 3.62, ALPHA: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 460/500 [20:47<01:50,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 460, REWARD: 182.46, XDIFF: 3.65, ALPHA: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 480/500 [21:44<00:57,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 480, REWARD: 184.89, XDIFF: 3.70, ALPHA: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [22:42<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 500, REWARD: 3.68, XDIFF: 0.07, ALPHA: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ReplayBuffer = ReplayBufferClass(buffer_limit, device=device)\n",
    "CriticOne = CriticClass(obs_dim=env.odim, a_dim=env.adim, h_dims=[256, 256], out_dim=1, lr_critic=lr_critic, device=device).to(device)\n",
    "CriticTwo = CriticClass(obs_dim=env.odim, a_dim=env.adim, h_dims=[256, 256], out_dim=1, lr_critic=lr_critic, device=device).to(device)\n",
    "CriticOneTarget = CriticClass(obs_dim=env.odim, a_dim=env.adim, h_dims=[256, 256], out_dim=1, lr_critic=lr_critic, device=device).to(device)\n",
    "CriticTwoTarget = CriticClass(obs_dim=env.odim, a_dim=env.adim, h_dims=[256, 256], out_dim=1, lr_critic=lr_critic, device=device).to(device)\n",
    "Actor = ActorClass(obs_dim=env.odim, h_dims=[256, 256], out_dim=env.adim, max_torque=max_torque, init_alpha=init_alpha, lr_actor=lr_actor, lr_alpha=lr_alpha, device=device).to(device)\n",
    "CriticOneTarget.load_state_dict(CriticOne.state_dict())\n",
    "CriticTwoTarget.load_state_dict(CriticTwo.state_dict())\n",
    "\n",
    "for episode in tqdm.tqdm(range(1, max_episode+1)):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    reward_total = 0\n",
    "    reward_forward = 0\n",
    "    for i in range(epi_length):\n",
    "        if episode < random_episode+1:\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, reward, done, info = env.step(a)\n",
    "            if i == epi_length-1:\n",
    "                done = True\n",
    "            ReplayBuffer.put((s, a, reward, s_prime, done))\n",
    "        else:\n",
    "            a, log_prob = Actor(torch.from_numpy(s).float().to(device))\n",
    "            s_prime, reward, done, info = env.step(a.detach().cpu().numpy())\n",
    "            if i == epi_length-1:\n",
    "                done = True\n",
    "            ReplayBuffer.put((s, a.detach().cpu().numpy(), reward, s_prime, done))\n",
    "        reward_total += reward\n",
    "        reward_forward += info['reward_forward']\n",
    "        s = s_prime \n",
    "        if done is True:\n",
    "            break   \n",
    "        \n",
    "        # Update\n",
    "        if ReplayBuffer.size() > 10000:\n",
    "            for j in range(n_gradient_step_per_update): \n",
    "                mini_batch = ReplayBuffer.sample(batch_size)\n",
    "                td_target = get_target(Actor, CriticOneTarget, CriticTwoTarget, gamma=gamma, mini_batch=mini_batch, device=device)\n",
    "                CriticOne.train(td_target, mini_batch)\n",
    "                CriticTwo.train(td_target, mini_batch)\n",
    "                Actor.train(CriticOne, CriticTwo, target_entropy=-env.adim, mini_batch=mini_batch)\n",
    "                CriticOne.soft_update(tau=tau, net_target=CriticOneTarget)\n",
    "                CriticTwo.soft_update(tau=tau, net_target=CriticTwoTarget)\n",
    "    x_diff = env.sim.data.qpos[0] \n",
    "    \n",
    "    if episode % print_interval == 0:\n",
    "        print(\"EPISODE: {}, REWARD: {:.2f}, XDIFF: {:.2f}, ALPHA: {:.4f}\".format(episode, reward_total, x_diff, Actor.log_alpha.exp()))\n",
    "    \n",
    "    if episode % save_interval == 0:\n",
    "        if not os.path.exists(\"results/weights\"):\n",
    "            os.makedirs(\"results/weights\")\n",
    "        torch.save(Actor.state_dict(), \"results/weights/sac_model_weights_{}.pth\".format(episode))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Eval` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 3\n",
    "eval_episode = 400\n",
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapbot(4legs) Environment\n",
      "Obs Dim: [103] Act Dim: [8] dt:[0.02] Condition:[None]\n",
      "ctrl_coef:[0] head_coef:[0]\n",
      "REWARD: 162.75 XDIFF: 3.25\n",
      "REWARD: 168.02 XDIFF: 3.36\n",
      "REWARD: 176.91 XDIFF: 3.54\n"
     ]
    }
   ],
   "source": [
    "env = Snapbot4EnvClass(xml_path='../snapbot_env/xml/snapbot_4/robot_4_', render_mode=None)\n",
    "epi_length = 300\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "s = env.reset()\n",
    "Actor = ActorClass(\n",
    "                    obs_dim=env.odim, \n",
    "                    out_dim=env.adim,\n",
    "                    max_torque=1,\n",
    "                    init_alpha=0.1,\n",
    "                    lr_actor=0,\n",
    "                    lr_alpha=0.1,\n",
    "                    device=device).to(device)\n",
    "Actor.load_state_dict(torch.load('results/weights/sac_model_weights_{}.pth'.format(eval_episode), map_location=device))\n",
    "\n",
    "for sample_idx in range(n_sample):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    q_pos = []\n",
    "    rewards = 0\n",
    "    for i in range(epi_length):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        q_pos.append(env.get_joint_pos_deg())\n",
    "        a, _ = Actor(torch.from_numpy(s).float())\n",
    "        s_prime, reward, done, info = env.step(a.detach().cpu().numpy())\n",
    "        rewards += reward\n",
    "        s = s_prime\n",
    "        if done:\n",
    "            break\n",
    "    print(\"REWARD: {:.2f} XDIFF: {:.2f}\".format(rewards, env.sim.data.qpos[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
