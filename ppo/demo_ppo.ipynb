{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hansoo/.pyenv/versions/3.9.15/envs/fire/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, gym, os, sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(''))))\n",
    "from torch.distributions import Normal\n",
    "from snapbot_env.class_snapbot import Snapbot4EnvClass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Util` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2torch(x: np.ndarray,\n",
    "             dtype: torch.dtype = torch.float32,\n",
    "             device: torch.device = torch.device('cpu')):\n",
    "    return torch.from_numpy(x).type(dtype).to(device)\n",
    "\n",
    "def build_mlp(in_dim: int,\n",
    "              h_dims: list,\n",
    "              h_actv: nn.Module):\n",
    "    layers = []\n",
    "    for i in range(len(h_dims)):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(in_dim, h_dims[i]))\n",
    "        else:\n",
    "            layers.append(nn.Linear(h_dims[i-1], h_dims[i]))\n",
    "        layers.append(h_actv)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def calc_gae(reward_batch: torch.Tensor,\n",
    "             val_batch: torch.Tensor,\n",
    "             done,\n",
    "             last_val,\n",
    "             gamma: float,\n",
    "             lmbda: float):\n",
    "    advantage_batch = np.zeros(shape=(reward_batch.shape[0], 1), dtype=np.float32)\n",
    "    \n",
    "    advantage = 0.0\n",
    "    for idx in reversed(range(reward_batch.shape[0])):\n",
    "        if idx == reward_batch.shape[0]-1:\n",
    "            next_done = 1 - done\n",
    "            next_val = last_val\n",
    "        else:\n",
    "            next_done = 1\n",
    "            next_val = val_batch[idx+1]\n",
    "        delta = reward_batch[idx] + gamma * next_val * next_done - val_batch[idx]\n",
    "        advantage = delta + gamma * lmbda * next_done * advantage\n",
    "        advantage_batch[idx] = advantage\n",
    "    return_batch = advantage_batch + val_batch\n",
    "    return return_batch, advantage_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PPOBuffer` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBufferClass:\n",
    "    def __init__(self,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "                 buffer_size):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.obs_buffer = np.zeros(shape=(buffer_size, obs_dim), dtype=np.float32)\n",
    "        self.act_buffer = np.zeros(shape=(buffer_size, act_dim), dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(shape=(buffer_size, 1), dtype=np.float32)\n",
    "        self.log_prob_buffer = np.zeros(shape=(buffer_size, 1), dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(shape=(buffer_size, 1), dtype=np.float32)\n",
    "        self.advantage_buffer = np.zeros(shape=(buffer_size, 1), dtype=np.float32)\n",
    "        self.val_buffer = np.zeros(shape=(buffer_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.random_generator = np.random.default_rng()\n",
    "        self.start_idx, self.pointer = 0, 0\n",
    "        \n",
    "    def put(self,\n",
    "            obs,\n",
    "            act,\n",
    "            reward,\n",
    "            val,\n",
    "            log_prob):\n",
    "        self.obs_buffer[self.pointer] = obs\n",
    "        self.act_buffer[self.pointer] = act\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.val_buffer[self.pointer] = val\n",
    "        self.log_prob_buffer[self.pointer] = log_prob\n",
    "        self.pointer += 1\n",
    "    \n",
    "    def get_gae_batch(self,\n",
    "                      gamma,\n",
    "                      lmbda,\n",
    "                      done,\n",
    "                      last_val):\n",
    "        path_slice = slice(self.start_idx, self.pointer)\n",
    "        val_mini_buffer = self.val_buffer[path_slice]\n",
    "        \n",
    "        self.return_buffer[path_slice], self.advantage_buffer[path_slice] = calc_gae(\n",
    "                                                                                    self.reward_buffer[path_slice],\n",
    "                                                                                    val_mini_buffer,\n",
    "                                                                                    done,\n",
    "                                                                                    last_val,\n",
    "                                                                                    gamma,\n",
    "                                                                                    lmbda\n",
    "                                                                                    )\n",
    "        self.start_idx = self.pointer\n",
    "    \n",
    "    def get_mini_batch(self,\n",
    "                       mini_batch_size):\n",
    "        assert mini_batch_size <= self.pointer\n",
    "        indices = np.arange(self.pointer)\n",
    "        self.random_generator.shuffle(indices)\n",
    "        \n",
    "        split_indices = []\n",
    "        point = mini_batch_size\n",
    "        while point < self.pointer:\n",
    "            split_indices.append(point)\n",
    "            point += mini_batch_size\n",
    "        \n",
    "        temp_data = {\n",
    "                    'obs': np.split(self.obs_buffer[indices], split_indices),\n",
    "                    'act': np.split(self.act_buffer[indices], split_indices),\n",
    "                    'reward': np.split(self.reward_buffer[indices], split_indices),\n",
    "                    'val': np.split(self.val_buffer[indices], split_indices),\n",
    "                    'log_prob': np.split(self.log_prob_buffer[indices], split_indices),\n",
    "                    'return': np.split(self.return_buffer[indices], split_indices),\n",
    "                    'advantage': np.split(self.advantage_buffer[indices], split_indices)\n",
    "                    }\n",
    "        \n",
    "        data = []\n",
    "        for k in range(len(temp_data['obs'])):\n",
    "            data.append({\n",
    "                        'obs': temp_data['obs'][k],\n",
    "                        'action': temp_data['act'][k],\n",
    "                        'reward': temp_data['reward'][k],\n",
    "                        'val': temp_data['val'][k],\n",
    "                        'log_prob': temp_data['log_prob'][k],\n",
    "                        'return': temp_data['return'][k],\n",
    "                        'advantage': temp_data['advantage'][k]\n",
    "                        })\n",
    "        return data\n",
    "\n",
    "    def clear(self):\n",
    "        self.start_idx, self.pointer = 0, 0\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Actor` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorClass(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_torque: float,\n",
    "                 obs_dim: int,\n",
    "                 h_dims: list,\n",
    "                 act_dim: int,\n",
    "                 h_actv: nn.Module,\n",
    "                 mu_actv: nn.Module,\n",
    "                 lr_actor: float,\n",
    "                 ):\n",
    "        super(ActorClass, self).__init__()\n",
    "        self.max_torque = max_torque\n",
    "        self.layers = build_mlp(in_dim=obs_dim, h_dims=h_dims, h_actv=h_actv)\n",
    "        self.mu_head = nn.Linear(h_dims[-1], act_dim)\n",
    "        self.mu_actv = mu_actv\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                obs: torch.Tensor):\n",
    "        x = self.layers(obs)\n",
    "        if self.mu_actv is not None:\n",
    "            mu = self.mu_actv(self.mu_head(x))\n",
    "        else:\n",
    "            mu = self.mu_head(x)\n",
    "            mu = (mu+1)*self.max_torque - self.max_torque\n",
    "        return mu\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Critic` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticClass(nn.Module):\n",
    "    def __init__(self,\n",
    "                 obs_dim: int,\n",
    "                 h_dims: list,\n",
    "                 val_dim: int,\n",
    "                 h_actv: nn.Module,\n",
    "                 out_actv: nn.Module,\n",
    "                 lr_critic: float):\n",
    "        super(CriticClass, self).__init__()\n",
    "        self.layers = build_mlp(in_dim=obs_dim, h_dims=h_dims, h_actv=h_actv)\n",
    "        self.val_head = nn.Linear(h_dims[-1], val_dim)\n",
    "        self.out_actv = out_actv\n",
    "        if self.out_actv is not None:\n",
    "            self.out_actv = out_actv\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def forward(self,\n",
    "                obs):\n",
    "        x = self.layers(obs)\n",
    "        val = self.val_head(x)\n",
    "        if self.out_actv is not None:\n",
    "            val = self.out_actv(val)\n",
    "        return val\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PPO` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOClass(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_torque: float,\n",
    "                 obs_dim: int,\n",
    "                 act_dim: int,\n",
    "                 h_dims: list,\n",
    "                 gamma: float,\n",
    "                 lmbda: float,\n",
    "                 lr_actorcritic: float,\n",
    "                 clip_ratio: float,\n",
    "                 value_coef: float,\n",
    "                 entropy_coef: float,\n",
    "                 max_grad: float\n",
    "                 ):\n",
    "        super(PPOClass, self).__init__()\n",
    "        \n",
    "        self.max_torque = max_torque\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef   \n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad = max_grad\n",
    "        \n",
    "        self.actor = ActorClass(max_torque=max_torque,obs_dim=obs_dim,h_dims=h_dims,act_dim=act_dim,h_actv=nn.ReLU(),mu_actv=None,lr_actor=lr_actorcritic)\n",
    "        self.critic = CriticClass(obs_dim=obs_dim,h_dims=h_dims,val_dim=1,h_actv=nn.ReLU(),out_actv=None,lr_critic=lr_actorcritic)\n",
    "        self.log_std = nn.Parameter(torch.ones(act_dim) * torch.log(torch.tensor((1.0))), requires_grad=True)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr_actorcritic)\n",
    "    \n",
    "    def forward(self,\n",
    "                obs: torch.Tensor):\n",
    "        mu = self.actor(obs)\n",
    "        val = self.critic(obs)\n",
    "        dist = Normal(mu, torch.exp(self.log_std))\n",
    "        return dist, val\n",
    "    \n",
    "    def get_action(self,\n",
    "                   obs):\n",
    "        obs_torch = torch.unsqueeze(torch.FloatTensor(obs), 0)\n",
    "        dist, val = self.forward(obs_torch)\n",
    "        action = dist.sample()\n",
    "        log_prob = torch.sum(dist.log_prob(action), dim=1)\n",
    "        return action[0].detach().numpy(), torch.squeeze(log_prob).detach().numpy(), torch.squeeze(val).detach().numpy()\n",
    "        \n",
    "    def get_val(self,\n",
    "                  obs):\n",
    "        obs_torch = torch.unsqueeze(torch.FloatTensor(obs), 0)\n",
    "        dist, val = self.forward(obs_torch)\n",
    "        return torch.squeeze(val).detach().numpy()\n",
    "\n",
    "    def eval_action(self,\n",
    "                    obs_batch,\n",
    "                    act_batch):\n",
    "        obs_torch = obs_batch.clone().detach()\n",
    "        action_torch = act_batch.clone().detach()\n",
    "        dist, val = self.forward(obs_torch)\n",
    "        log_prob = dist.log_prob(action_torch)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "        entropy = dist.entropy()\n",
    "        return log_prob, val, entropy\n",
    "    \n",
    "    def update(self,\n",
    "               obs_batch,\n",
    "               act_batch,\n",
    "               log_prob_batch,\n",
    "               advantage_batch,\n",
    "               return_batch):\n",
    "        new_log_prob_batch, val_batch, entropy = self.eval_action(obs_batch, act_batch)\n",
    "        ratio = torch.exp(new_log_prob_batch - log_prob_batch)\n",
    "        \n",
    "        surr1 = ratio * advantage_batch\n",
    "        surr2 = torch.clip(ratio, 1.0-self.clip_ratio, 1.0+self.clip_ratio) * advantage_batch\n",
    "        actor_loss = -torch.mean(torch.min(surr1, surr2)) + self.entropy_coef*entropy.mean()\n",
    "        critic_loss = self.value_coef * torch.mean((val_batch-return_batch)**2)        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.parameters(), self.max_grad)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return actor_loss.detach(), critic_loss.detach(), total_loss.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Env` & `Hyperparameter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapbot(4legs) Environment\n",
      "Obs Dim: [103] Act Dim: [8] dt:[0.02] Condition:[None]\n",
      "ctrl_coef:[0] head_coef:[0]\n"
     ]
    }
   ],
   "source": [
    "env = Snapbot4EnvClass(xml_path='../snapbot_env/xml/snapbot_4/robot_4_', render_mode=None)\n",
    "max_iter = 200\n",
    "max_step = 300\n",
    "buffer_size = 2048\n",
    "mini_batch_size = 64\n",
    "n_step_per_update=buffer_size\n",
    "k_epoch = 10\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "h_dims = [128, 128]\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "lr_actorcritic = 3e-4\n",
    "clip_ratio = 0.2\n",
    "value_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO = PPOClass(max_torque=2.0,\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                h_dims=h_dims,\n",
    "                gamma=gamma,\n",
    "                lmbda=lmbda,\n",
    "                lr_actorcritic=lr_actorcritic,\n",
    "                clip_ratio=clip_ratio,\n",
    "                value_coef=value_coef,\n",
    "                entropy_coef=entropy_coef,\n",
    "                max_grad=max_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Main` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1, AVG_REWARD=0.31, ACTOR_LOSS=-0.03, CRITIC_LOSS=0.26, TOTAL_LOSS=0.23\n",
      "Iter=2, AVG_REWARD=5.13, ACTOR_LOSS=-0.04, CRITIC_LOSS=0.23, TOTAL_LOSS=0.19\n",
      "Iter=3, AVG_REWARD=1.35, ACTOR_LOSS=-0.04, CRITIC_LOSS=0.27, TOTAL_LOSS=0.23\n",
      "Iter=4, AVG_REWARD=8.62, ACTOR_LOSS=-0.05, CRITIC_LOSS=0.24, TOTAL_LOSS=0.19\n",
      "Iter=5, AVG_REWARD=2.29, ACTOR_LOSS=-0.05, CRITIC_LOSS=0.24, TOTAL_LOSS=0.19\n",
      "Iter=6, AVG_REWARD=7.74, ACTOR_LOSS=-0.05, CRITIC_LOSS=0.23, TOTAL_LOSS=0.17\n",
      "Iter=7, AVG_REWARD=9.83, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.22, TOTAL_LOSS=0.16\n",
      "Iter=8, AVG_REWARD=10.37, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.24, TOTAL_LOSS=0.18\n",
      "Iter=9, AVG_REWARD=12.38, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.19, TOTAL_LOSS=0.13\n",
      "Iter=10, AVG_REWARD=10.57, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.21, TOTAL_LOSS=0.16\n",
      "Iter=11, AVG_REWARD=10.57, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.20, TOTAL_LOSS=0.14\n",
      "Iter=12, AVG_REWARD=16.02, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.20, TOTAL_LOSS=0.14\n",
      "Iter=13, AVG_REWARD=13.26, ACTOR_LOSS=-0.07, CRITIC_LOSS=0.21, TOTAL_LOSS=0.14\n",
      "Iter=14, AVG_REWARD=17.81, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.24, TOTAL_LOSS=0.18\n",
      "Iter=15, AVG_REWARD=22.59, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.37, TOTAL_LOSS=0.31\n",
      "Iter=16, AVG_REWARD=21.59, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.32, TOTAL_LOSS=0.26\n",
      "Iter=17, AVG_REWARD=21.55, ACTOR_LOSS=-0.07, CRITIC_LOSS=0.29, TOTAL_LOSS=0.22\n",
      "Iter=18, AVG_REWARD=25.37, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.35, TOTAL_LOSS=0.29\n",
      "Iter=19, AVG_REWARD=30.19, ACTOR_LOSS=-0.07, CRITIC_LOSS=0.27, TOTAL_LOSS=0.20\n",
      "Iter=20, AVG_REWARD=27.33, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.45, TOTAL_LOSS=0.39\n",
      "Iter=21, AVG_REWARD=24.69, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.28, TOTAL_LOSS=0.22\n",
      "Iter=22, AVG_REWARD=25.31, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.38, TOTAL_LOSS=0.32\n",
      "Iter=23, AVG_REWARD=26.63, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.53, TOTAL_LOSS=0.46\n",
      "Iter=24, AVG_REWARD=37.23, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.38, TOTAL_LOSS=0.33\n",
      "Iter=25, AVG_REWARD=22.25, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.58, TOTAL_LOSS=0.52\n",
      "Iter=26, AVG_REWARD=32.97, ACTOR_LOSS=-0.07, CRITIC_LOSS=0.50, TOTAL_LOSS=0.43\n",
      "Iter=27, AVG_REWARD=34.98, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.73, TOTAL_LOSS=0.67\n",
      "Iter=28, AVG_REWARD=40.67, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.97, TOTAL_LOSS=0.91\n",
      "Iter=29, AVG_REWARD=48.48, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.67, TOTAL_LOSS=0.61\n",
      "Iter=30, AVG_REWARD=44.91, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.76, TOTAL_LOSS=0.70\n",
      "Iter=31, AVG_REWARD=42.00, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.51, TOTAL_LOSS=0.45\n",
      "Iter=32, AVG_REWARD=43.24, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.74, TOTAL_LOSS=0.68\n",
      "Iter=33, AVG_REWARD=39.91, ACTOR_LOSS=-0.06, CRITIC_LOSS=1.04, TOTAL_LOSS=0.99\n",
      "Iter=34, AVG_REWARD=36.75, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.91, TOTAL_LOSS=0.85\n",
      "Iter=35, AVG_REWARD=62.35, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.78, TOTAL_LOSS=0.72\n",
      "Iter=36, AVG_REWARD=51.88, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.93, TOTAL_LOSS=0.87\n",
      "Iter=37, AVG_REWARD=60.51, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.81, TOTAL_LOSS=0.75\n",
      "Iter=38, AVG_REWARD=68.77, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.83, TOTAL_LOSS=0.77\n",
      "Iter=39, AVG_REWARD=51.65, ACTOR_LOSS=-0.05, CRITIC_LOSS=0.81, TOTAL_LOSS=0.76\n",
      "Iter=40, AVG_REWARD=56.01, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.85, TOTAL_LOSS=0.79\n",
      "Iter=41, AVG_REWARD=62.59, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.67, TOTAL_LOSS=0.61\n",
      "Iter=42, AVG_REWARD=53.37, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.13, TOTAL_LOSS=1.08\n",
      "Iter=43, AVG_REWARD=59.52, ACTOR_LOSS=-0.05, CRITIC_LOSS=0.98, TOTAL_LOSS=0.93\n",
      "Iter=44, AVG_REWARD=76.99, ACTOR_LOSS=-0.06, CRITIC_LOSS=0.88, TOTAL_LOSS=0.82\n",
      "Iter=45, AVG_REWARD=48.56, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.82, TOTAL_LOSS=1.78\n",
      "Iter=46, AVG_REWARD=50.03, ACTOR_LOSS=-0.06, CRITIC_LOSS=2.14, TOTAL_LOSS=2.08\n",
      "Iter=47, AVG_REWARD=71.03, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.74, TOTAL_LOSS=1.70\n",
      "Iter=48, AVG_REWARD=79.37, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.59, TOTAL_LOSS=1.53\n",
      "Iter=49, AVG_REWARD=65.44, ACTOR_LOSS=-0.05, CRITIC_LOSS=2.15, TOTAL_LOSS=2.10\n",
      "Iter=50, AVG_REWARD=66.54, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.85, TOTAL_LOSS=1.81\n",
      "Iter=51, AVG_REWARD=81.75, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.12, TOTAL_LOSS=1.07\n",
      "Iter=52, AVG_REWARD=72.59, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.72, TOTAL_LOSS=1.67\n",
      "Iter=53, AVG_REWARD=65.21, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.65, TOTAL_LOSS=1.60\n",
      "Iter=54, AVG_REWARD=69.55, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.18, TOTAL_LOSS=1.14\n",
      "Iter=55, AVG_REWARD=87.14, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.12, TOTAL_LOSS=1.08\n",
      "Iter=56, AVG_REWARD=100.66, ACTOR_LOSS=-0.02, CRITIC_LOSS=0.92, TOTAL_LOSS=0.90\n",
      "Iter=57, AVG_REWARD=99.40, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.20, TOTAL_LOSS=1.16\n",
      "Iter=58, AVG_REWARD=104.67, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.17, TOTAL_LOSS=1.12\n",
      "Iter=59, AVG_REWARD=102.61, ACTOR_LOSS=-0.05, CRITIC_LOSS=1.05, TOTAL_LOSS=1.01\n",
      "Iter=60, AVG_REWARD=111.13, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.07, TOTAL_LOSS=1.04\n",
      "Iter=61, AVG_REWARD=101.62, ACTOR_LOSS=-0.02, CRITIC_LOSS=0.99, TOTAL_LOSS=0.97\n",
      "Iter=62, AVG_REWARD=106.16, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.11, TOTAL_LOSS=1.07\n",
      "Iter=63, AVG_REWARD=115.06, ACTOR_LOSS=-0.06, CRITIC_LOSS=1.44, TOTAL_LOSS=1.38\n",
      "Iter=64, AVG_REWARD=114.69, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.07, TOTAL_LOSS=1.03\n",
      "Iter=65, AVG_REWARD=122.85, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.07, TOTAL_LOSS=1.05\n",
      "Iter=66, AVG_REWARD=122.84, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.02, TOTAL_LOSS=0.99\n",
      "Iter=67, AVG_REWARD=105.02, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.69, TOTAL_LOSS=1.64\n",
      "Iter=68, AVG_REWARD=111.97, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.54, TOTAL_LOSS=1.50\n",
      "Iter=69, AVG_REWARD=125.59, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.23, TOTAL_LOSS=1.20\n",
      "Iter=70, AVG_REWARD=125.15, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.02, TOTAL_LOSS=0.98\n",
      "Iter=71, AVG_REWARD=112.14, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.69, TOTAL_LOSS=1.66\n",
      "Iter=72, AVG_REWARD=109.20, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.36, TOTAL_LOSS=2.33\n",
      "Iter=73, AVG_REWARD=109.21, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.94, TOTAL_LOSS=1.91\n",
      "Iter=74, AVG_REWARD=116.65, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.91, TOTAL_LOSS=1.89\n",
      "Iter=75, AVG_REWARD=114.50, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.17, TOTAL_LOSS=2.14\n",
      "Iter=76, AVG_REWARD=105.73, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.59, TOTAL_LOSS=2.56\n",
      "Iter=77, AVG_REWARD=131.08, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.22, TOTAL_LOSS=1.21\n",
      "Iter=78, AVG_REWARD=135.95, ACTOR_LOSS=0.00, CRITIC_LOSS=0.92, TOTAL_LOSS=0.93\n",
      "Iter=79, AVG_REWARD=136.47, ACTOR_LOSS=-0.02, CRITIC_LOSS=0.82, TOTAL_LOSS=0.80\n",
      "Iter=80, AVG_REWARD=146.26, ACTOR_LOSS=-0.00, CRITIC_LOSS=0.78, TOTAL_LOSS=0.78\n",
      "Iter=81, AVG_REWARD=127.69, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.49, TOTAL_LOSS=1.46\n",
      "Iter=82, AVG_REWARD=141.30, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.61, TOTAL_LOSS=1.57\n",
      "Iter=83, AVG_REWARD=142.21, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.45, TOTAL_LOSS=2.40\n",
      "Iter=84, AVG_REWARD=155.58, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.44, TOTAL_LOSS=1.42\n",
      "Iter=85, AVG_REWARD=151.61, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.51, TOTAL_LOSS=1.49\n",
      "Iter=86, AVG_REWARD=165.47, ACTOR_LOSS=-0.02, CRITIC_LOSS=0.78, TOTAL_LOSS=0.76\n",
      "Iter=87, AVG_REWARD=167.17, ACTOR_LOSS=-0.03, CRITIC_LOSS=0.89, TOTAL_LOSS=0.86\n",
      "Iter=88, AVG_REWARD=140.97, ACTOR_LOSS=0.00, CRITIC_LOSS=2.39, TOTAL_LOSS=2.39\n",
      "Iter=89, AVG_REWARD=169.87, ACTOR_LOSS=0.01, CRITIC_LOSS=0.76, TOTAL_LOSS=0.77\n",
      "Iter=90, AVG_REWARD=145.60, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.71, TOTAL_LOSS=2.67\n",
      "Iter=91, AVG_REWARD=131.44, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.56, TOTAL_LOSS=2.53\n",
      "Iter=92, AVG_REWARD=121.82, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.55, TOTAL_LOSS=4.50\n",
      "Iter=93, AVG_REWARD=155.53, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.44, TOTAL_LOSS=4.41\n",
      "Iter=94, AVG_REWARD=124.13, ACTOR_LOSS=-0.02, CRITIC_LOSS=3.40, TOTAL_LOSS=3.38\n",
      "Iter=95, AVG_REWARD=132.56, ACTOR_LOSS=-0.04, CRITIC_LOSS=5.55, TOTAL_LOSS=5.50\n",
      "Iter=96, AVG_REWARD=164.70, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.89, TOTAL_LOSS=2.85\n",
      "Iter=97, AVG_REWARD=144.95, ACTOR_LOSS=0.01, CRITIC_LOSS=2.91, TOTAL_LOSS=2.91\n",
      "Iter=98, AVG_REWARD=149.23, ACTOR_LOSS=-0.05, CRITIC_LOSS=4.91, TOTAL_LOSS=4.86\n",
      "Iter=99, AVG_REWARD=121.27, ACTOR_LOSS=-0.05, CRITIC_LOSS=7.01, TOTAL_LOSS=6.96\n",
      "Iter=100, AVG_REWARD=136.85, ACTOR_LOSS=-0.02, CRITIC_LOSS=3.94, TOTAL_LOSS=3.93\n",
      "Iter=101, AVG_REWARD=152.03, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.85, TOTAL_LOSS=2.81\n",
      "Iter=102, AVG_REWARD=146.96, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.54, TOTAL_LOSS=2.50\n",
      "Iter=103, AVG_REWARD=132.02, ACTOR_LOSS=-0.05, CRITIC_LOSS=4.98, TOTAL_LOSS=4.93\n",
      "Iter=104, AVG_REWARD=154.85, ACTOR_LOSS=0.01, CRITIC_LOSS=2.01, TOTAL_LOSS=2.02\n",
      "Iter=105, AVG_REWARD=165.72, ACTOR_LOSS=0.08, CRITIC_LOSS=1.36, TOTAL_LOSS=1.44\n",
      "Iter=106, AVG_REWARD=153.27, ACTOR_LOSS=-0.03, CRITIC_LOSS=3.08, TOTAL_LOSS=3.04\n",
      "Iter=107, AVG_REWARD=145.56, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.56, TOTAL_LOSS=2.53\n",
      "Iter=108, AVG_REWARD=171.05, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.55, TOTAL_LOSS=1.52\n",
      "Iter=109, AVG_REWARD=163.27, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.85, TOTAL_LOSS=1.82\n",
      "Iter=110, AVG_REWARD=184.08, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.16, TOTAL_LOSS=1.15\n",
      "Iter=111, AVG_REWARD=179.95, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.43, TOTAL_LOSS=1.43\n",
      "Iter=112, AVG_REWARD=181.56, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.36, TOTAL_LOSS=1.35\n",
      "Iter=113, AVG_REWARD=169.35, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.74, TOTAL_LOSS=2.71\n",
      "Iter=114, AVG_REWARD=176.91, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.82, TOTAL_LOSS=1.81\n",
      "Iter=115, AVG_REWARD=170.52, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.16, TOTAL_LOSS=2.13\n",
      "Iter=116, AVG_REWARD=169.31, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.18, TOTAL_LOSS=2.15\n",
      "Iter=117, AVG_REWARD=183.03, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.19, TOTAL_LOSS=1.18\n",
      "Iter=118, AVG_REWARD=188.63, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.57, TOTAL_LOSS=1.57\n",
      "Iter=119, AVG_REWARD=168.54, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.31, TOTAL_LOSS=2.27\n",
      "Iter=120, AVG_REWARD=167.11, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.83, TOTAL_LOSS=2.79\n",
      "Iter=121, AVG_REWARD=188.28, ACTOR_LOSS=-0.05, CRITIC_LOSS=2.65, TOTAL_LOSS=2.60\n",
      "Iter=122, AVG_REWARD=175.47, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.53, TOTAL_LOSS=2.48\n",
      "Iter=123, AVG_REWARD=182.16, ACTOR_LOSS=-0.03, CRITIC_LOSS=3.43, TOTAL_LOSS=3.39\n",
      "Iter=124, AVG_REWARD=189.90, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.15, TOTAL_LOSS=2.11\n",
      "Iter=125, AVG_REWARD=183.60, ACTOR_LOSS=0.04, CRITIC_LOSS=2.65, TOTAL_LOSS=2.69\n",
      "Iter=126, AVG_REWARD=171.82, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.15, TOTAL_LOSS=4.11\n",
      "Iter=127, AVG_REWARD=198.09, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.75, TOTAL_LOSS=1.74\n",
      "Iter=128, AVG_REWARD=189.98, ACTOR_LOSS=0.01, CRITIC_LOSS=2.21, TOTAL_LOSS=2.22\n",
      "Iter=129, AVG_REWARD=185.17, ACTOR_LOSS=0.04, CRITIC_LOSS=1.28, TOTAL_LOSS=1.32\n",
      "Iter=130, AVG_REWARD=139.16, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.19, TOTAL_LOSS=2.16\n",
      "Iter=131, AVG_REWARD=148.98, ACTOR_LOSS=-0.05, CRITIC_LOSS=2.98, TOTAL_LOSS=2.93\n",
      "Iter=132, AVG_REWARD=158.08, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.73, TOTAL_LOSS=2.69\n",
      "Iter=133, AVG_REWARD=162.10, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.41, TOTAL_LOSS=2.37\n",
      "Iter=134, AVG_REWARD=150.39, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.69, TOTAL_LOSS=2.64\n",
      "Iter=135, AVG_REWARD=179.69, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.90, TOTAL_LOSS=1.89\n",
      "Iter=136, AVG_REWARD=186.56, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.22, TOTAL_LOSS=1.21\n",
      "Iter=137, AVG_REWARD=145.81, ACTOR_LOSS=-0.04, CRITIC_LOSS=3.69, TOTAL_LOSS=3.65\n",
      "Iter=138, AVG_REWARD=157.88, ACTOR_LOSS=-0.04, CRITIC_LOSS=3.56, TOTAL_LOSS=3.52\n",
      "Iter=139, AVG_REWARD=172.04, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.01, TOTAL_LOSS=3.97\n",
      "Iter=140, AVG_REWARD=174.48, ACTOR_LOSS=-0.05, CRITIC_LOSS=3.08, TOTAL_LOSS=3.04\n",
      "Iter=141, AVG_REWARD=185.78, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.68, TOTAL_LOSS=2.65\n",
      "Iter=142, AVG_REWARD=198.57, ACTOR_LOSS=0.01, CRITIC_LOSS=1.27, TOTAL_LOSS=1.28\n",
      "Iter=143, AVG_REWARD=195.37, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.81, TOTAL_LOSS=1.79\n",
      "Iter=144, AVG_REWARD=165.66, ACTOR_LOSS=-0.05, CRITIC_LOSS=3.92, TOTAL_LOSS=3.87\n",
      "Iter=145, AVG_REWARD=216.50, ACTOR_LOSS=-0.02, CRITIC_LOSS=2.18, TOTAL_LOSS=2.16\n",
      "Iter=146, AVG_REWARD=178.09, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.47, TOTAL_LOSS=4.43\n",
      "Iter=147, AVG_REWARD=200.62, ACTOR_LOSS=-0.01, CRITIC_LOSS=3.15, TOTAL_LOSS=3.13\n",
      "Iter=148, AVG_REWARD=199.37, ACTOR_LOSS=0.05, CRITIC_LOSS=3.56, TOTAL_LOSS=3.61\n",
      "Iter=149, AVG_REWARD=174.57, ACTOR_LOSS=-0.02, CRITIC_LOSS=3.63, TOTAL_LOSS=3.61\n",
      "Iter=150, AVG_REWARD=203.02, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.32, TOTAL_LOSS=1.31\n",
      "Iter=151, AVG_REWARD=207.26, ACTOR_LOSS=0.06, CRITIC_LOSS=2.07, TOTAL_LOSS=2.13\n",
      "Iter=152, AVG_REWARD=217.05, ACTOR_LOSS=0.03, CRITIC_LOSS=0.73, TOTAL_LOSS=0.76\n",
      "Iter=153, AVG_REWARD=198.69, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.69, TOTAL_LOSS=1.69\n",
      "Iter=154, AVG_REWARD=181.13, ACTOR_LOSS=-0.04, CRITIC_LOSS=2.73, TOTAL_LOSS=2.69\n",
      "Iter=155, AVG_REWARD=184.24, ACTOR_LOSS=-0.03, CRITIC_LOSS=3.07, TOTAL_LOSS=3.04\n",
      "Iter=156, AVG_REWARD=212.48, ACTOR_LOSS=0.07, CRITIC_LOSS=1.51, TOTAL_LOSS=1.58\n",
      "Iter=157, AVG_REWARD=180.43, ACTOR_LOSS=-0.02, CRITIC_LOSS=3.80, TOTAL_LOSS=3.78\n",
      "Iter=158, AVG_REWARD=217.80, ACTOR_LOSS=0.07, CRITIC_LOSS=1.84, TOTAL_LOSS=1.91\n",
      "Iter=159, AVG_REWARD=215.25, ACTOR_LOSS=0.05, CRITIC_LOSS=2.33, TOTAL_LOSS=2.38\n",
      "Iter=160, AVG_REWARD=215.66, ACTOR_LOSS=0.21, CRITIC_LOSS=2.16, TOTAL_LOSS=2.37\n",
      "Iter=161, AVG_REWARD=215.82, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.70, TOTAL_LOSS=1.70\n",
      "Iter=162, AVG_REWARD=67.61, ACTOR_LOSS=-0.05, CRITIC_LOSS=6.46, TOTAL_LOSS=6.41\n",
      "Iter=163, AVG_REWARD=96.02, ACTOR_LOSS=-0.05, CRITIC_LOSS=4.99, TOTAL_LOSS=4.94\n",
      "Iter=164, AVG_REWARD=133.06, ACTOR_LOSS=-0.04, CRITIC_LOSS=4.58, TOTAL_LOSS=4.54\n",
      "Iter=165, AVG_REWARD=153.81, ACTOR_LOSS=-0.03, CRITIC_LOSS=4.43, TOTAL_LOSS=4.40\n",
      "Iter=166, AVG_REWARD=144.82, ACTOR_LOSS=-0.02, CRITIC_LOSS=3.83, TOTAL_LOSS=3.81\n",
      "Iter=167, AVG_REWARD=179.76, ACTOR_LOSS=-0.01, CRITIC_LOSS=2.15, TOTAL_LOSS=2.14\n",
      "Iter=168, AVG_REWARD=190.34, ACTOR_LOSS=0.07, CRITIC_LOSS=0.84, TOTAL_LOSS=0.91\n",
      "Iter=169, AVG_REWARD=208.38, ACTOR_LOSS=0.06, CRITIC_LOSS=1.17, TOTAL_LOSS=1.23\n",
      "Iter=170, AVG_REWARD=214.65, ACTOR_LOSS=-0.01, CRITIC_LOSS=0.97, TOTAL_LOSS=0.96\n",
      "Iter=171, AVG_REWARD=205.44, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.10, TOTAL_LOSS=2.07\n",
      "Iter=172, AVG_REWARD=210.45, ACTOR_LOSS=0.00, CRITIC_LOSS=1.38, TOTAL_LOSS=1.38\n",
      "Iter=173, AVG_REWARD=202.41, ACTOR_LOSS=0.03, CRITIC_LOSS=1.55, TOTAL_LOSS=1.58\n",
      "Iter=174, AVG_REWARD=215.93, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.10, TOTAL_LOSS=1.10\n",
      "Iter=175, AVG_REWARD=203.07, ACTOR_LOSS=0.07, CRITIC_LOSS=1.24, TOTAL_LOSS=1.31\n",
      "Iter=176, AVG_REWARD=137.58, ACTOR_LOSS=-0.04, CRITIC_LOSS=1.95, TOTAL_LOSS=1.91\n",
      "Iter=177, AVG_REWARD=145.17, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.48, TOTAL_LOSS=1.47\n",
      "Iter=178, AVG_REWARD=157.19, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.33, TOTAL_LOSS=1.31\n",
      "Iter=179, AVG_REWARD=161.11, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.71, TOTAL_LOSS=1.70\n",
      "Iter=180, AVG_REWARD=174.88, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.60, TOTAL_LOSS=1.57\n",
      "Iter=181, AVG_REWARD=194.57, ACTOR_LOSS=-0.01, CRITIC_LOSS=0.79, TOTAL_LOSS=0.78\n",
      "Iter=182, AVG_REWARD=201.33, ACTOR_LOSS=0.05, CRITIC_LOSS=0.95, TOTAL_LOSS=0.99\n",
      "Iter=183, AVG_REWARD=197.02, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.59, TOTAL_LOSS=1.59\n",
      "Iter=184, AVG_REWARD=203.29, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.46, TOTAL_LOSS=1.44\n",
      "Iter=185, AVG_REWARD=191.43, ACTOR_LOSS=-0.00, CRITIC_LOSS=1.35, TOTAL_LOSS=1.34\n",
      "Iter=186, AVG_REWARD=199.37, ACTOR_LOSS=-0.01, CRITIC_LOSS=0.97, TOTAL_LOSS=0.96\n",
      "Iter=187, AVG_REWARD=189.73, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.47, TOTAL_LOSS=1.43\n",
      "Iter=188, AVG_REWARD=217.28, ACTOR_LOSS=0.06, CRITIC_LOSS=0.77, TOTAL_LOSS=0.83\n",
      "Iter=189, AVG_REWARD=186.35, ACTOR_LOSS=-0.03, CRITIC_LOSS=1.66, TOTAL_LOSS=1.63\n",
      "Iter=190, AVG_REWARD=190.78, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.64, TOTAL_LOSS=1.62\n",
      "Iter=191, AVG_REWARD=204.59, ACTOR_LOSS=-0.02, CRITIC_LOSS=1.63, TOTAL_LOSS=1.61\n",
      "Iter=192, AVG_REWARD=196.91, ACTOR_LOSS=-0.02, CRITIC_LOSS=2.37, TOTAL_LOSS=2.35\n",
      "Iter=193, AVG_REWARD=196.70, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.44, TOTAL_LOSS=2.40\n",
      "Iter=194, AVG_REWARD=205.90, ACTOR_LOSS=-0.01, CRITIC_LOSS=2.32, TOTAL_LOSS=2.31\n",
      "Iter=195, AVG_REWARD=216.62, ACTOR_LOSS=-0.01, CRITIC_LOSS=1.77, TOTAL_LOSS=1.76\n",
      "Iter=196, AVG_REWARD=187.16, ACTOR_LOSS=-0.04, CRITIC_LOSS=3.28, TOTAL_LOSS=3.24\n",
      "Iter=197, AVG_REWARD=198.50, ACTOR_LOSS=-0.02, CRITIC_LOSS=2.28, TOTAL_LOSS=2.26\n",
      "Iter=198, AVG_REWARD=200.68, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.89, TOTAL_LOSS=2.86\n",
      "Iter=199, AVG_REWARD=194.19, ACTOR_LOSS=-0.03, CRITIC_LOSS=2.71, TOTAL_LOSS=2.68\n",
      "Iter=200, AVG_REWARD=208.59, ACTOR_LOSS=0.01, CRITIC_LOSS=2.36, TOTAL_LOSS=2.37\n"
     ]
    }
   ],
   "source": [
    "PPO = PPOClass(max_torque=2.0,\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                h_dims=h_dims,\n",
    "                gamma=gamma,\n",
    "                lmbda=lmbda,\n",
    "                lr_actorcritic=lr_actorcritic,\n",
    "                clip_ratio=clip_ratio,\n",
    "                value_coef=value_coef,\n",
    "                entropy_coef=entropy_coef,\n",
    "                max_grad=max_grad)\n",
    "\n",
    "PPOBuffer = PPOBufferClass(obs_dim=obs_dim,\n",
    "                            act_dim=act_dim,\n",
    "                            buffer_size=buffer_size)\n",
    "epi_reward = 0\n",
    "epi_cnt = 0\n",
    "iter_cnt = 0\n",
    "actor_loss_ls = []\n",
    "critic_loss_ls = []\n",
    "total_loss_ls = []\n",
    "mean_rewards = []\n",
    "\n",
    "obs = env.reset()\n",
    "n_step = 0\n",
    "\n",
    "for t in range(1, buffer_size*max_iter + 1):\n",
    "    n_step += 1\n",
    "    action, log_prob, val = PPO.get_action(obs)\n",
    "    real_action = np.clip(action, -PPO.max_torque, PPO.max_torque)\n",
    "    next_obs, reward, done, info = env.step(real_action)\n",
    "    \n",
    "    epi_reward += reward\n",
    "\n",
    "    PPOBuffer.put(obs, action, reward, val, log_prob)\n",
    "    obs = next_obs\n",
    "\n",
    "    if t % n_step_per_update == 0 or done or n_step == max_step:\n",
    "        if n_step == max_step:\n",
    "            epi_cnt += 1\n",
    "        if done:\n",
    "            epi_cnt += 1\n",
    "        last_val = PPO.get_val(obs)\n",
    "        PPOBuffer.get_gae_batch(gamma=PPO.gamma,\n",
    "                                lmbda=PPO.lmbda,\n",
    "                                done=done,\n",
    "                                last_val=last_val)\n",
    "        obs = env.reset()\n",
    "        n_step = 0\n",
    "    \n",
    "    if t % n_step_per_update == 0:\n",
    "        iter_cnt += 1\n",
    "        for _ in range(k_epoch):\n",
    "            mini_batch_data = PPOBuffer.get_mini_batch(mini_batch_size=mini_batch_size)\n",
    "            n_mini_batch = len(mini_batch_data)\n",
    "            for k in range(n_mini_batch):\n",
    "                obs_batch = mini_batch_data[k]['obs']\n",
    "                act_batch = mini_batch_data[k]['action']\n",
    "                log_prob_batch = mini_batch_data[k]['log_prob']\n",
    "                advantage_batch = mini_batch_data[k]['advantage']\n",
    "                advantage_batch = (advantage_batch - np.squeeze(np.mean(advantage_batch, axis=0))) / (np.squeeze(np.std(advantage_batch, axis=0)) + 1e-8)\n",
    "                return_batch = mini_batch_data[k]['return']\n",
    "                \n",
    "                obs_batch = np2torch(obs_batch)\n",
    "                act_batch = np2torch(act_batch)\n",
    "                log_prob_batch = np2torch(log_prob_batch)\n",
    "                advantage_batch = np2torch(advantage_batch)\n",
    "                return_batch = np2torch(return_batch)\n",
    "\n",
    "                actor_loss, critic_loss, total_loss = PPO.update(obs_batch, act_batch, log_prob_batch, advantage_batch, return_batch)\n",
    "                actor_loss_ls.append(actor_loss.numpy())\n",
    "                critic_loss_ls.append(critic_loss.numpy())\n",
    "                total_loss_ls.append(total_loss.numpy())\n",
    "        PPOBuffer.clear()      \n",
    "        \n",
    "        mean_ep_reward = epi_reward / epi_cnt\n",
    "        epi_reward, epi_cnt = 0, 0\n",
    "        print(f\"Iter={iter_cnt}, AVG_REWARD={mean_ep_reward:.2f}, ACTOR_LOSS={np.mean(actor_loss_ls):.2f}, CRITIC_LOSS={np.mean(critic_loss_ls):.2f}, TOTAL_LOSS={np.mean(total_loss_ls):.2f}\")\n",
    "        actor_loss_ls = []; critic_loss_ls = []; total_loss_ls = []; mean_rewards = []\n",
    "        if iter_cnt % 50 == 0:\n",
    "            if not os.path.exists(\"./results/weights\"):\n",
    "                os.makedirs(\"./results/weights\")\n",
    "            torch.save(PPO.actor.state_dict(), f\"./results/weights/snapbot_actor_{iter_cnt}.pth\")\n",
    "            torch.save(PPO.critic.state_dict(), f\"./results/weights/snapbot_critic_{iter_cnt}.pth\")\n",
    "            torch.save(PPO.log_std, f\"./results/weights/snapbot_log_std_{iter_cnt}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
